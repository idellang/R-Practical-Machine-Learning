---
title: "Week 3"
author: "Me"
date: "1/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Predicting with trees
- split variables into groups
- evaluate homogeneity within groups
- split again if necessary

Pros
- easy to interpet
- better performance in nonlinear

Cons
- can lead to overfitting without pruning/cv
- harder to estimate uncertainty
- results may vary depending on variables

Basic algo
- starts with all variables in one big group
- find the variable that best separates outcome
- divide into two groups on that split
- within each split, find the best variable that separates the outcome
- continue until the groups are too small.

Measures of impurity
- Misclassification error (0 = perfect, .5 = no purity)
- Gini index
- information gain/ deviance - 0 is perfect, 1 is no purity

Example iris
```{r}
data("iris")
library(ggplot2)
names(iris)
```
```{r}
table(iris$Species)
```
Create training and test set
```{r}
in_train = createDataPartition(y = iris$Species, p = .7, list = F)
training = iris[in_train,]
testing = iris[in_train,]
dim(training)
dim(testing)
```

Plot petal width against sepal with
```{r}
qplot(Petal.Width, Sepal.Width, color = Species, data = training)
```
Iris petal widths/ sepal width
```{r}
library(caret)
mod_fit = train(Species ~ ., method = 'rpart', data = training)
print(mod_fit$finalModel)
```

Plot tree

```{r}
plot(mod_fit$finalModel, uniform = TRUE, main = 'Tree')
text(mod_fit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
```
Prettier plot
```{r}
library(rattle)
fancyRpartPlot(mod_fit$finalModel)
```

Predict new values
```{r}
pred = predict(mod_fit, newdata= testing)
confusionMatrix(pred, testing$Species)
```
Notes
- classification trees are non linear and use interactions between variables
- data transformation is less important
- trees can be used for regression

## Bagging - bootstrap aggregating
- average model that has better balance on bias and variance

Idea
1. Resample and calculate predictions
2. average or majority vote
Notes
- similar bias
- reduced variance
- more useful for nonlinear functions.

Ozone data
```{r}
ozone = read.table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/ozone.data', header = TRUE)
ozone = ozone[order(ozone$ozone),]
head(ozone)
```


Bagged loess
```{r}
ll = matrix(NA, nrow = 10, ncol = 155)

for (i in 1:10){
      ss = sample(1:dim(ozone)[1], replace = T)
      ozone0 = ozone[ss, ]
      ozone0 = ozone0[order(ozone0$ozone),]
      loess0 = loess(temperature ~ ozone, data = ozone0, span = .2)
      ll[i,] =  predict(loess0, newdata = data.frame(ozone = 1:155))
}


```

```{r}
plot(ozone$ozone, ozone$temperature, pch = 19, cex = .5)
for (i in 1:10){lines(1:155, ll[i,], col = 'grey', lwd = 2)}
lines(1:155, apply(ll, 2, mean), col = 'red', lwd = 2)
```
Red line is bagged loess curve. 

Baggign results to lower variability with similar bias

Some models perform bagging for you, in train consider method options
- bagEarth
- treebag
- bagFDA

you can bag any model using bag function

Create your own bagging
```{r}
predictors = data.frame(ozone = ozone$ozone)
temperature = ozone$temperature
treebag = bag(predictors, temperature, B = 10,
              bagControl = bagControl(fit = ctreeBag$fit,
                                      predict = ctreeBag$pred,
                                      aggregate = ctreeBag$aggregate))
```

```{r}
plot(ozone$ozone, ozone$temperature, col= 'lightgrey', pch = 19)
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch = 19, col = 'red')
points(ozone$ozone, predict(treebag, predictors), pch = 19, col = 'blue')
```
Parts of bagging
```{r}
ctreeBag$fit
```
Uses ctree function.
```{r}
ctreeBag$pred
```

```{r}
ctreeBag$aggregate
```

Notes
- useful for non linear
- often used for trees - extension is random forest
- several models use bagging in caret's train function.

## Random forest
Idea
1. Bootstrap samples
2. At each split, bootstrap variables. 
3. Grow multiple trees and vote.

Pros
- accuracy
Cons
- Speed
- interpretability
- overfitting (use CV)

Iris data
```{r}
data("iris")
library(ggplot2)
library(caret)

```

```{r}
in_train = createDataPartition(y = iris$Species, p = .7, list = F)
training = iris[in_train,]
testing = iris[-in_train,]
```

```{r}
mod_fit = train(Species ~., data = training, method = 'rf', prox = TRUE)
mod_fit
```

Get a single tree
```{r}
library(randomForest)
getTree(mod_fit$finalModel, k = 2)
```

Class centers
```{r}
irisP = classCenter(training[,c(3,4)], training$Species, mod_fit$finalModel$proximity)
irisP = as.data.frame(irisP)
irisP$Species = rownames(irisP)

p = qplot(Petal.Width, Petal.Length, col = Species, data = training)
p + geom_point(aes(x = Petal.Width, y = Petal.Length, col = Species), size = 5, shape = 4, data = irisP)
```


Predicting new values
```{r}
pred = predict(mod_fit, testing)
testing$predRight = pred == testing$Species
table(pred, testing$Species)
```

```{r}
qplot(Petal.Width, Petal.Length, color = predRight, data = testing)
```

Missed are those that are in between two separate classes. 

Notes
- RF is one of top performing algorithm.
- use cross validation. 

## Boosting
Idea
1. takes lot of weak predictors
2. weight them and add them
3. get a stronger predictor

1. Start with hk classifiers ex: all possible trees, all possible regression models, all possible cutoffs
2. create a classifier that combines classification functions
- goal is minimize error
- iterative, select h at each step
- calculate weights based on errors
- upweight missed classification and select next h

Most famous is adaboost

- Boosting can be done with any subset of classifiers
- one large subclass is gradient boosting
- R has multiple boosting libraries
   - gbm : boosting with trees
   - mboost : model based boosting
   - ada : statistical boosting on additive logistic regression
   - gamBoost : for generalized additive models

```{r}
library(ISLR)
library(ggplot2)
library(caret)

Wage = subset(Wage, select = -c(logwage))
in_train = createDataPartition(y = Wage$wage, p = .7, list = F)

training = Wage[in_train,]
testing = Wage[-in_train,]
```

```{r}
mod_fit = train(wage ~ ., method = 'gbm', data = training, verbose = F)
```

```{r}
qplot(predict(mod_fit,testing), wage, data = testing)
```

## Model based prediction

Basic idea
1. Assumes the data follows a probabilistic model
2. use bayes' theorem to identify optimal classifiers based on probabilistic model

Pros:
- can take advantage of structure of data
- may be computationally convenient
- are reasonably accurate on real problems 

Cons
- make additional assumptions
- when the model is incorrect, get reduced accuracy

Model based approach
1 build parametric model or conditional distribution P(y = k|X = x)
2 typical approach is bayes theorem. 
3. typical prior probabilities are set in advance. 
4. Common choice for f(x) is gaussian. 
5. Estimate parameters mean and sd from the data
6. Classify to the class with highest probability. 

Classifying using the model
- LDA assumes f(x) is multivariate gaussian w/ same covariances
- QDA assums f(x) is multiavariate gaussian w/ different covariances
- model based prediction assumes more complicated version for covariance matrix
- Naive Bayes assumes independence between features.Works for binary or categorical. For text classification. 

```{r}
data(iris)
library(ggplot2)
names(iris)
```

```{r}
library(caret)

in_train = createDataPartition(y = iris$Species, p = .7, list = F)
training = iris[in_train,]
testing = iris[-in_train,]
dim(training)
dim(testing)
```
```{r}
mod_lda = train(Species ~., data = training, method = 'lda')
mod_nb = train(Species ~., data = training, method = 'nb')
```

```{r}
plda = predict(mod_lda, testing)
pnb = predict(mod_nb, testing)

table(plda, pnb)
```

```{r}
equal_prediction = (plda == pnb)

qplot(Petal.Width, Sepal.Width, color = equal_prediction, data= testing)
```
## Quiz

Q1
```{r}
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
```

```{r}
head(segmentationOriginal)
```

```{r}
set.seed(125)
in_train = segmentationOriginal$Case == 'Train'

training = segmentationOriginal[in_train,]
testing = segmentationOriginal[-in_train,]

mod_fit = train(Class ~ ., data = training, method = 'rpart')
```

```{r}
library(rattle)
fancyRpartPlot(mod_fit$finalModel)
```


Q2. Higher K means lower variance


Q3

```{r}
library(pgmm)
data(olive)
olive = olive[,-1]
```

```{r}
library(caret)
mod_fit = train(Area ~ ., data = olive, method = 'rpart' )
```
```{r}
newdata = as.data.frame(t(colMeans(olive)))
predict(mod_fit, newdata)
```

Q4. 
```{r}
SAheart <- read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data",
                      sep=",",head=T,row.names=1)

set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
```



```{r}
library(caret)
set.seed(13234)
mod_fit = train(factor(chd) ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
```

```{r}
trainSA$chd = as.factor(trainSA$chd)
testSA$chd = as.factor(testSA$chd)
```


```{r}
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}

confusionMatrix(trainSA$chd, predict(mod_fit))
confusionMatrix(testSA$chd, predict(mod_fit, testSA))
```
Maybe because I used other file for Q4. My misclassification rate is not on the choices


Q5.

```{r}
vowel.train <- read.csv(file = "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train") 

vowel.train <- vowel.train[,-1] 

vowel.test <- read.csv("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test") 

vowel.test <- vowel.test[,-1]
```

```{r}
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
```

```{r}
set.seed(33833)
rf_fit = randomForest::randomForest(y ~., data=  vowel.train)
```

```{r}
library(tidyverse)
varImp(rf_fit) %>%
   arrange(desc(Overall))
```



